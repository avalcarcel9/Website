---
title: "Using the CCEB High Performance Computing Cluster: Batch Jobs and Normal Sessions"
author: "Alessandra Valcarcel"
date: 2019-07-13T21:01:01-06:00
categories: ["Computing"]
tags: ["Cluster", "CCEB", "LSF", "IBM"]
output: 
  bookdown::html_document2:
    number_sections: false
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(out.width = '80%',
                      fig.align = 'center', 
                      echo=TRUE,
                      eval = FALSE, 
                      warning=FALSE, 
                      message=FALSE)
```

# Introduction

This is the eighth blog post in a series of articles about using the CCEB cluster. An overview of the series is available [here](www.alessandravalcarcel.com/blog/2019-04-13-clusterintro/). This post focuses on normal sessions and submitting batch jobs. 

Initializing a normal session and submitting a batch job is an advanced topic but you'll still be requesting appropriate system parameters using the LSF `bsub` command. We will again focus on the `bsub` for a normal session and common advanced options associated with the `bsub` to request more cores for parallelization, force memory constraints on your session, and request certain machines. We will also set up the source Rscript to allow for efficient batch job submission. 

A note on vocabulary. A batch job is used to refer to the DEFINE BATCH JOB USING WIKIPEDIA HERE. The way in which a batch job is run or submit is through a normal session on the cluster. 

Unlike the interactive session, normal sessions are requested and submit through your `bsub` command. Once the jobs is submit, it will either run or error. These processes are not interactive or dynamic. After the submission, you have to wait for the job to finish as a success or with an exit code (error). In the next blog post, we will cover ways to check in on a submit batch job through the normal session.

A normal session is no different than literally waiting in a queue. You submit your job to run. The __master host__ assesses your job request based on the availability of machines, memory, cores, etc and your requested specifications. The job is then sent to run, pend, or killed. The benefit of this approach is that the cluster can self-manage the resources and jobs submit. It is possible that some of your jobs will be run while others pend.

Complex code, code that takes a long time to run, or requires a lot of memory should always be submit to the normal queue. The system maintainers prefer that we submit jobs as often as possible as normal rather than interactive to allow the master host to self regulate efficiently. It is slightly more cumbersome to de-bug though so spend time checking your code thoroughly before hand in an interactive session.

# Normal Session Setup

Like anything on the cluster you need to `ssh` onto the submission host. From the submission host you can use the `bsub` command to submit a normal session or batch job.

A normal session, is more slightly more annoying to setup than an interactive session. You first will need to put your code on the cluster. Recall I use [fetch](https://fetchsoftworks.com/) a file transfer client free for students and academics, to do this. Then you'll need to submit the `bsub` command. 

With a normal session and batch jobs it is common to put a number of the unchanged portions of the `bsub` into a shell script. A shell script, or .sh file, is a place to store commonly used `bsub` options so you don't have to run them every time within the interactive `bsub` command. This may be confusing now but we will go though an example.

# Normal `bsub` Options

Many of the normal submission options are the same as an interactive session but we'll cover them in the context of a normal job here. 

The most basic `bsub` to submit a normal job is as follows:

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]" -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

- The normal session is requested since we use `-q cceb_normal` rather than `-q cceb_interactive`.
- This submits a job with the name `jobname` to run the `Rscript` found in `/project/taki3/amv/cluster/code/normal_example.R`. You can specify whatever you'd like for this. For example, `sims`, `tapas`, `alisjob`.
- `1-n` jobs or iterations are to be run.
- Since a normal session is not interactive, the job has to report some details about what happened. Details regarding the job and messages from printed from code are returned in the `output.txt` and since the full path is specified this file is saved in `/project/taki3/amv/cluster/code`.
- This does not specify how many jobs to run simultaneously so the cluster will allocate the number running based on how free machines are.
    - In a normal/batch session, we are limited by PMACS to a total of 50 cores running simultaneously so keep that in mind when submitting jobs. The host won't give you more than that even if you specify more.

Some common additions include:

1. `-n #` - to request multiple cores for each job

```{bash}
bsub -q cceb_normal -J "jobname[1-n]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

- If you have another layer of parallelization inside your R script you need to specify the requested cores here and the `mc.cores` inside the R function. 
- Again, `-n #` cannot exceed 50 by PMACs usage restrictions. If you're on another cluster or queue this may be possible. For example, when I use the `taki` queue I have access to all the cores in his cluster queue.

2. `-R "span[hosts=1]"` - to request that the total number of cores you are requesting in a single iteration of the job be all on the same host. This is only true for a single job in the batch not across all batches. For example, job 1 may span `silver01` while job 2 may span `silver02`.

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]" -R "span[hosts=1]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

3. `-R "rusage[mem=####]"` - to request the amount of memory required for your a single iteration in your job to run. An iteration of the job will not run until this amount of memory is available and once running you will retain/reserve the memory. Note, the memory is in megabytes.

- Only request the amount of memory you think your job requires or else other users will not have it available to them! I will show in the next blog post how to quality control and detect the amount of memory your jobs are using to input an educated amount.

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]" -R "span[hosts=1] rusage[mem=5000]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R

bsub -q cceb_normal -J "jobname[1-n]" -R "rusage[mem=5000]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

4. `-m "machinename"` - to request a specific host by name.

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]" -m "silver02" -R "span[hosts=1] rusage[mem=5000]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R

bsub -q cceb_normal -J "jobname[1-n]" -m "silver02" -R "rusage[mem=5000]" -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

It is not advisable to request specific machines on the CCEB cluster. This over-rides the master hosts allocation and result in core and memory issues. Additionally, cluster maintenance is scheduled it could error and kill all of your jobs.

5. `-M ####` - to kill the job if it exceeds the #### memory amount.

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]" -m "silver02" -R "span[hosts=1] rusage[mem=5000]" -M 10000 -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R

bsub -q cceb_normal -J "jobname[1-n]" -m "silver02" -R "rusage[mem=5000]" -M 10000 -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

6. `"jobname[1-n]%N"` -this command names the job, sets the total `n` number of iterations to run, and also sets the number of iterations `%N` that are possible to run simultaneously.

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-n]%5" -m "silver02" -R "span[hosts=1] rusage[mem=5000]" -M 10000 -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R

bsub -q cceb_normal -J "jobname[1-n]%5" -m "silver02" -R "rusage[mem=5000]" -M 10000 -n 8 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

- The 50 core limit still applies here so `-n #` * `%N` cannot exceed 50.
- This will specify that N jobs run simultaneously and each job requires `-n #` cores. Do your math to be sure you don't use more than 50.

# Work Flow

1. Create and de-bug the R code you will run as a batch job. It is easiest to create this code in an interactive session to be sure there are no bugs and it will not error out. A good programmer will write the code as a function and execute the function in the R script. If one iteration of the R script runs in an interactive session the most likely you're ready for the normal batch job.
2. Use a file transfer client (i.e. Fetch) to put the R code on the cluster.
3. `ssh` onto the cluster.
4. Use a `bsub` to submit the batch job and initiate a normal session.
5. Check your code is submit using `bjobs`. After you submit a job on the cluster you can check what iterations are running or pending using `bjobs`. You'll type `bjobs` from the command line on either the submission host or on an execution host. For example, you can run `bjobs` after `ssh`'ing or after `ssh`'ing and `bsub`'ing into an interactive session. 
6. Let your code run. The code/job will either pend, run, or error.
7. Check your output file to verify the job ended successfully.

## `bjob` Example

PUT AN EXAMPLE OF RUNNING bjobs where to run it, what it returns, and a screen shot.

# Examples

Below I provide 3 unique examples of running code using a normal session. These are the most common job types you will run when using a normal session. Each example is executing the same code but utilizing the cluster in different ways. Therefore, the code is not exactly the same but the results should all be the same assuming the same seed is specified across the methods. I've adapted the example code provided in the [Advanced Topics for Interactive Sessions](INSERT SITE HERE) blog post.

## Example 1: Classic Batch Job 

This example runs the code provided below as 100 separate jobs in a normal session. After ssh'ing onto takim/scisub run the following:

### R Code

Copy and paste the code below into an R script. Change the paths in the code to match those related to your machine. Save the script with the title `normal_example1.R`. Use a transfer client and put this file on the cluster. Note, you should specify the file paths to where you put the script and would like to save output on the cluster.

```{r}
library(lme4)
library(parallel)

normal_example1 <- function(i, wd = '/project/taki3/amv/cluster/') {
  message('Number of cores detected ', Sys.getenv('LSB_DJOB_NUMPROC'))
  message(paste0('Sampling data for iter ', i))
  # Randomly sample without replacement iris data
  iris_sample = dplyr::sample_n(tbl = iris, size = 75, replace = FALSE)
  message(paste0('Calculating model for iter ', i))
  # Fit linear mixed effects model
  model = lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length, data = iris_sample)
  # If the directory doesn't exist for job create it
  if(dir.exists(paste0(wd, 'linear_models')) == FALSE){
    dir.create(paste0(wd, 'linear_models'))
  }
  message(paste0('Ssaving model for iter ', i))
  # Save each iteration of results
  saveRDS(object = model, file = paste0(wd, "linear_models/model_norm1_session_", i, ".rds"))
  message(paste0('Completed iter ', i))
  # Return the fitted model
  return(model)
}

# Set a master seed
set.seed(23)
# Create a vector of seeds for the normal batch job
i = as.numeric(Sys.getenv("LSB_JOBINDEX"))
# If i = 0 set it to 1
# Useful for when you're de-bugging code in an interactive session
if(i == 0){
  i = 1
}
#Create a vector of seeds for array job
iter = 100
seed_vec = sample(1:100000, iter, replace=F)
set.seed(seed_vec[i])

# Run the example function using 1000 iterations on 8 cores
normal_example1(i = i, wd = '/project/taki3/amv/cluster/')
```

EXPLAIN THIS IS LIKE RUNNING A FOR LOOP BUT IT ISN"T 
NOTE THE SPECIAL THINGS ABOUT THIS CODE:
- `i = as.numeric(Sys.getenv("LSB_JOBINDEX"))`
- `# If i = 0 set it to 1 # Useful for when you're de-bugging code in an interactive session if(i == 0){i = 1}`
- `iter = 100`
- `seed_vec = sample(1:100000, iter, replace=F)`
- `set.seed(seed_vec[i])`













To be sure that this function is ready to submit as a batch job and there are no bugs run a single iteration of the `normal_example1` function. You can just run everything after the comment `# Set a master seed` in an interactive session.

### `bsub`

After you have your R code ready to execute and saved on the cluster `ssh` onto the submission host and run the following `bsub` command.

```{bash}
bsub -q cceb_normal -J "norm1[1-100]%20" -R "rusage[mem=500]" -M 1000 -o /project/taki3/amv/cluster/code/normal_example1_output.txt Rscript /project/taki3/amv/cluster/code/normal_example1.R
```

This command is going to run the `normal_example1.R` code as a batch job. 100 iterations or jobs were submit to the queue with the name `norm`. I have forced this example to run only 20 jobs (`%20`) simultaneously or in parallel simply for example purposes. An iteration will not be submit to run until 500 megabytes are free and will error out if a single job exceeds 1000 megabytes. The output from both the LSF normal session jobs and anything output from R to `/project/taki3/amv/cluster/code/normal_example1_output.txt`.

## Example 2: A Single Parallel Batch Job

### R Code

### `bsub`

## Example 3: Parallelize Code Inside of a Batch Job

### R Code

### `bsub`















### Normal Job Sample Code

Inside of a normal job script you can have the same function with different parameters or different functions all together. For example you can write a function to generate data and then a separate function to analyze the data.

These examples are really basic and provide a simple example in order to help you better understand the `bsub`. You'll need to file transfer the code to the cluster before running. I put the `code` directory in `/project/taki3/amv/cluster/`<!--Section \@ref(sample code download section) provides all the sample code.-->

#### Example 1

This example runs the code provided in "normal_example.R" as 100 separate jobs. After ssh'ing onto takim/scisub run the following:

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-100]%20" -R "rusage[mem=500]" -M 1000 -o /project/taki3/amv/cluster/code/example_output.txt Rscript /project/taki3/amv/cluster/code/normal_example.R
```

- I am not running another layer of parallelization inside of the code so no need to request multiple cores
- Jobs are not dependent so I do not request the job to span only 1 host
- I do not request a specific machine
- I do set the code to run as 100 jobs to with 20 running at a time and specify the memory usage and kill

On your own take a look at the example_output.txtso you familiarize with the format and what it looks like. <!-- I included the output files in the code available in @ref()--> The output.txt continues on when you keep submitting by default and does not write over the previous version. If your job failed and you re-ran it and it succeeded you will see the failure at the top of the document but if you scroll down you'll see the later submission success. I like to keep this to compare information but if you prefer you can write over it or delete it.

#### Example 2

This example will run the "normal_example2.R" code. This example calls a single `mclapply` to run in as a single normal batch job on 8 cores. After ssh'ing onto takim/scisub run the following:

```{bash, eval = FALSE}
bsub -q cceb_normal -J "jobname[1-1]" -R "rusage[mem=5000]" -M 10000 -n 8 -o /project/taki3/amv/cluster/code/example_output2.txt Rscript /project/taki3/amv/cluster/code/normal_example2.R
```

- Only 1 job is submit because the R script runs the 1000 iterations on 8 cores
- Because the single job requires 8 cores you have to request -n 8 cores in the `bsub`

#### Comparison of Examples

__Example \ref@(example-1)__ is the best way to parallelize majority of the time. You can make __ Example \ref@(example-1)__ more complicated and break up jobs to work in parallel inside of your batch job if you need more cores for each job. __Example \ref@(example-1)__ avoids some of the memory issues you can run into when using __Example \ref@(example-2)__. You can also control the number of jobs and cores running at the same time somewhat easier. 

For simulations where you are changing parameters __Example \ref@(example-1)__ still works. You could put mclapply statements with lots of parameters set in your R code and call seeds using the `as.numeric(Sys.getenv("LSB_JOBINDEX"))` all to be sent as a batch job.

I pick __Example \ref@(example-2)__ over __Example \ref@(example-1)__ when I need to submit lots of jobs (i.e. more than 5000) since this could over-whelm the cluster or when I really need a large amount of cores to be working on a single common task. This is common in big data imaging/genetics but less common for other statistical fields with "small" data.





# Shell Scripts (`.sh`) for Submission

# Summary

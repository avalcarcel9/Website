---
title: "Using the CCEB High Performance Computing Cluster: Terminology and Specifications"
author: "Alessandra Valcarcel"
date: 2019-04-27T21:01:01-06:00
categories: ["Computing"]
tags: ["Cluster", "CCEB", "LSF", "IBM"]
output: 
  bookdown::html_document2:
    number_sections: false
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This is the fourth blog post in a series of articles about using the CCEB cluster. An overview of the series is available <a href="www.alessandravalcarcel.com/blog/2019-04-13-clusterintro/">here</a>. This post focuses on the general terminology surrounding high performance computing clusters and an overview of the CCEB cluster specifications.</p>
<p>Most of the information presented in this article was taken from IBM resources <a href="https://www.ibm.com/support/knowledgecenter/en/SSETD4_9.1.2/lsf_foundations/lsf_introduction_to.html">here</a> and Microsoft <a href="https://blogs.technet.microsoft.com/windowshpc/2008/04/14/how-that-nodesocketcore-thing-works/">here</a>. You should explore these sites on your own for specific details.</p>
</div>
<div id="cluster-versus-grid" class="section level1">
<h1>Cluster Versus Grid</h1>
<p><strong>Cluster computing</strong> uses several nodes that are made to run as a single entity. These nodes are connected by a local area network (LAN). A cluster is homogeneous in that they have all the same hardware and operating systems.</p>
<p><strong>Grid computing</strong> is the segregation of resources across multiple sites. Grids are are heterogeneous. The computers that are part of a grid can run different operating systems, have different hardware, and be in different geographical locations.</p>
<p><strong>Cloud computing</strong> is a new computing paradigm which provides a large pool of dynamic, scalable, and virtual resources to run processes. Think <a href="https://aws.amazon.com/">Amazon Web Services</a>.</p>
<p><strong>Grid computing</strong> is sometimes distinguished from conventional high-performance computing systems such as <strong>cluster computing</strong> in that grid computers have each node set to perform a different task/application or may have clusters in different geographical locations. A grid is a cluster but a cluster is not necessarily a grid.</p>
<p>Our CCEB resources were previously housed in different buildings meeting the spatial definition of a grid, but they were recently all moved into the same location. The operating system is the same across the entire machine but different machines are able to run different software programs (i.e. SAS, MATLAB, R). Technically, I believe the CCEB high performance computing resources are categorized as <strong>grid computing</strong>. Personally, I use cluster to reference the machine though. Others use grid. To be honest, for our purposes, the language of how we speak of the CCEB High Performance Computing (HPC) resources doesn’t matter. Both won’t affect how effectively we Google questions or discuss the resources among ourselves.</p>
<p>If you want to read more, this content was mostly taken from a really nice article in the International Journal of Advanced Research in Computer and Communication Engineering and <a href="https://www.wikipedia.org/">Wikipedia</a>. The links are provided below:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Grid_computing">Grid Definitions on Wikipedia</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computer_cluster">Cluster Definitions on Wikipedia</a></li>
<li><a href="https://pdfs.semanticscholar.org/5e6e/9b4b7f4d986bc9f1246198c50c8d43d2d695.pdf">Article discussing the subtle differences between Cloud Computing, Grid Computing, and Cluster Computing</a>.</li>
</ul>
</div>
<div id="hosts" class="section level1">
<h1>Hosts</h1>
<p><a href="https://www.ibm.com/support/knowledgecenter/en/SSETD4_9.1.3/lsf_users_guide/hosts_about.html" class="uri">https://www.ibm.com/support/knowledgecenter/en/SSETD4_9.1.3/lsf_users_guide/hosts_about.html</a></p>
<p>Hosts on the cluster perform different functions.</p>
<ul>
<li><p><strong>Master host</strong>: A server host acts as the overall coordinator for the cluster, doing all job scheduling and dispatch.</p></li>
<li><p><strong>Server host</strong>: A host that submits and runs jobs.</p></li>
<li><p><strong>Client host</strong>: A host that only submits jobs and tasks.</p></li>
<li><p><strong>Execution host</strong>: A host that runs jobs and tasks.</p></li>
<li><p><strong>Submission host</strong>: A host from which jobs and tasks are submitted.</p></li>
</ul>
<p>Typically, we only interact with <strong>server</strong>, <strong>client</strong>, <strong>execution</strong>, and <strong>submission</strong> hosts. The <strong>master</strong> host is working in the background after we submit jobs. In our department, we most commonly refer to all of these generically as <strong>host</strong> and don’t clarify which type of <strong>host</strong>. It can get confusing so for this document I’ll try to always clarify.</p>
<p>If you simply <code>ssh</code> onto the CCEB cluster you are put onto a <strong>submission host</strong>. No jobs can be run on this host directly but are submit and sent to run on other <strong>execution hosts</strong>. If you are part of a different queue, for example Taki or <a href="https://www.med.upenn.edu/pennsive/personnel.html">PennSIVE’s</a> group, then they use a <strong>server host</strong>, takim, so when you <code>ssh</code> onto the cluster your jobs are submit from there but can also be run on content on takim. Once you submit a job from these hosts, you tasks are run on <strong>execution hosts</strong>.</p>
<div id="nodes-sockets-and-cores" class="section level3">
<h3>Nodes, Sockets, and Cores</h3>
<p>A <strong>node</strong> (a.k.a. host, machine, computer) refers to an entire compute node. Each node contains 1 or more sockets. Notice, this is synonymous with host/machine.</p>
<p>A <strong>socket</strong> (a.k.a. numa node) refers to collection of cores with a direct pipe to memory. Each socket contains 1 or more cores. Note that this does not necessarily refer to a physical socket, but rather to the memory architecture of the machine, which will depend on your chip vendor. I pretty much never think about this level or use this language.</p>
<p>A <strong>core</strong> (a.k.a. processor, cpu, cpu core, logical processor) refers to a single processing unit capable of performing computations. A <strong>core</strong> is the smallest unit of allocation available in high performance computing.</p>
<p>When you <code>bsub</code> an interactive session with 1 core requested by definition you are using 1 core on 1 node or host. When you submit a normal job you can request multiple cores and unless specified they may be across multiple hosts/machines.</p>
</div>
<div id="job" class="section level2">
<h2>Job</h2>
<p>A <strong>job</strong> is the unit of work (i.e. your code) that is running on the cluster. A <strong>job</strong> is submit using a command. The <strong>master host</strong> schedules, controls, and tracks the job according to configured policies. <strong>Jobs</strong> can be complex problems, simulation scenarios, extensive calculations, or anything that needs compute power.</p>
<p>This language often gets confusing because we refer to the entire thing running as a batch or array job (i.e. all 1000 of your simulations running) and we refer to each 1000 iterations as single jobs. If you ever get confused just ask for clarification.</p>
<p>When you submit a <strong>job</strong> it goes into a <strong>job slot</strong> or a bucket from which a single unit of work is assigned on the grid system. Hosts can be configured with multiple <strong>job slots</strong> and you can dispatch jobs from queues until all the <strong>job slots</strong> are filled. You can correlate <strong>job slots</strong> with the total number of CPUs in the cluster.</p>
<p>Each job is assigned a number. You can use this number to check on the memory of your job, kill or stop your job, or include when emailing issues to PMACs.</p>
</div>
<div id="queue" class="section level2">
<h2>Queue</h2>
<p>A <strong>queue</strong> is a cluster-wide container for jobs. All jobs wait in queues until they are scheduled and dispatched to hosts. <strong>Queues</strong> do not correspond to individual hosts; each queue can use all <strong>server hosts</strong> on the cluster, or a configured subset of the <strong>server hosts</strong>. When you submit a job to a <strong>queue</strong>, you do not need to specify an <strong>execution host</strong>. The <strong>master host</strong> dispatches the job to the best available <strong>execution host</strong> on the cluster to run that job. You can specify an <strong>execution host</strong> if you want but this is only useful for quality control.</p>
<p>Queues implement different job scheduling and control policies.</p>
</div>
<div id="layout" class="section level2">
<h2>“Layout”</h2>
<div class="figure" style="text-align: center"><span id="fig:clusthardware"></span>
<img src="img/terminology/cluster_hardware.png" alt="Pictoral representation of the hardware and layout of the grid. This graphic is not the accurate physical layout but simply provided as an example." width="100%" />
<p class="caption">
Figure 1: Pictoral representation of the hardware and layout of the grid. This graphic is not the accurate physical layout but simply provided as an example.
</p>
</div>
<ul>
<li>The <strong>submission host</strong> also has nodes but cannot compute on this host. A <strong>server host</strong> can also be used to submit jobs but it is not a good idea to use this for running code. Imagine if a <strong>server host</strong> crashed then all dependent <strong>execute hosts</strong> would crash and everyone running jobs in your queue would be very angry.</li>
</ul>
<p>The machine that you are put on after <code>ssh</code>-ing with <code>ssh pennkey@scisub</code> is simply a <strong>submission host</strong>. You are not permitted to run any processes on this host but rather have to <code>bsub</code> onto another machine. On the other hand, Taki’s queue puts you onto a <strong>server host</strong>. This host can both submit jobs and also run jobs. We can compute on takim but it is dangerous.</p>
<p>We will talk about the <code>bsub</code> command in the next few blog posts but this will be how you get onto an execute host to actually run code and jobs.</p>
</div>
</div>
<div id="cluster-specs" class="section level1">
<h1>PMACS High Performance Computing (HPC) Grids</h1>
<p>These specifications were true as of April 22, 2019. The internal wiki for up to date information can be found <a href="https://dbe.med.upenn.edu/secure/wiki/index.php?title=Linux_Grid_Help">here</a>.</p>
<p>PMACS Limited Performance Computing (LPC) cluster is a new grid (IBM Spectrum) and the cluster is comprised a variety of hosts running CentOS Linux. Currently, two Linux hosting servers are available to submit jobs to Grid and/or run applications in an Interactive session. The CCEB HPC Statistical Grid has been run and supported over the past 15+ years by PMACS. The CCEB HPC is an LSF (“LSF”, short for load sharing facility) software. LSF is an <a href="https://www.ibm.com/support/knowledgecenter/en/SSETD4/product_welcome_platform_lsf.html">IBM platform</a>.</p>
<ul>
<li><p>This is industry-leading enterprise-class software that distributes work across existing heterogeneous IT resources to create a shared, scalable, and fault-tolerant infrastructure, that delivers faster, more reliable workload performance and reduces cost.</p></li>
<li><p>LSF balances load, allocates resources, and provides access to those resources.</p></li>
</ul>
<p>This Statistical HPC is composed of a total of 352 available cores (144 Matlab; 80 SAS; 144 Stata; and 524 unrestricted R). More information about machine specification and memory is available through the <a href="https://dbe.med.upenn.edu/secure/wiki/index.php?title=Linux_Grid_Help">Wiki</a>.</p>
<p>The current setup defaults to assigning one core for each job submitted to the HPC Grid, although proper set-up and usage of Matlab pools and R parallel packages will allow up to 12 cores to be used concurrently for a job.</p>
<p>Any one user may only use up to 50 cores (e.g., 50 1-core jobs) running concurrently on the grid.</p>
</div>
